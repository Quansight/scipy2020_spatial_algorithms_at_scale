{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dask Geohash Sorted\n",
    "\n",
    "import logging\n",
    "import time\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from shapely.geometry import Polygon, box\n",
    "from polygon_geohasher.polygon_geohasher import polygon_to_geohashes, geohashes_to_polygon\n",
    "import geohash\n",
    "from functools import reduce\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import dask.dataframe as dd\n",
    "from distributed import LocalCluster, Client\n",
    "\n",
    "cluster = LocalCluster(#silence_logs=logging.ERROR,\n",
    "                       dashboard_address=':8790',\n",
    "                       n_workers=2,\n",
    "                       threads_per_worker=2,\n",
    "                       memory_limit='5 GB')\n",
    "client = Client(cluster)\n",
    "client\n",
    "\n",
    "base_path = Path('../../')\n",
    "contiguous_us_bounding_box = box(-124.848974, 24.396308, -66.885444, 49.384358)\n",
    "\n",
    "# load contiguous us data\n",
    "df = dd.read_parquet(base_path / 'data/contiguous_us_sorted_geohash4.parquet')\n",
    "df.head(2)\n",
    "\n",
    "%%time\n",
    "# Save various size subsets of the zip code data\n",
    "zips_1 = gpd.read_file(base_path / f'data/zip_codes/zips_1.geojson').loc[:, ['geometry']]\n",
    "zips_10 = gpd.read_file(base_path / f'data/zip_codes/zips_10.geojson').loc[:, ['geometry']]\n",
    "zips_100 = gpd.read_file(base_path / f'data/zip_codes/zips_100.geojson').loc[:, ['geometry']]\n",
    "zips_1000 = gpd.read_file(base_path / f'data/zip_codes/zips_1000.geojson').loc[:, ['geometry']]\n",
    "zips_10000 = gpd.read_file(base_path / f'data/zip_codes/zips_10000.geojson').loc[:, ['geometry']]\n",
    "\n",
    "# Point in Polygon Test\n",
    "\n",
    "# filter function\n",
    "def spatial_join(large_data_df, zip_codes_gdf):\n",
    "    if large_data_df.empty:\n",
    "        print('empty')\n",
    "    crs = \"epsg:4326\"\n",
    "    large_data_gdf = gpd.GeoDataFrame(large_data_df,\n",
    "                                      geometry=gpd.points_from_xy(large_data_df.longitude,\n",
    "                                                                  large_data_df.latitude),\n",
    "                                      crs=crs)\n",
    "    rdf = gpd.sjoin(large_data_gdf, zip_codes_gdf, how='inner', op='within').drop(['index_right'], axis=1)\n",
    "    if rdf.empty:\n",
    "        print(rdf.columns)\n",
    "        return dd.from_pandas(pd.DataFrame([], columns=['latitude', 'longitude', 'geometry']), npartitions=1)\n",
    "    return rdf\n",
    "#     if not rdf.empty():\n",
    "#         return rdf\n",
    "#     else:\n",
    "#         print(\"None\")\n",
    "#         return \n",
    "\n",
    "\n",
    "\n",
    "num_partitions = df.npartitions\n",
    "geohash_precision = 4\n",
    "num_polygons = []\n",
    "time_sec = []\n",
    "num_result_points = []\n",
    "num_points = None\n",
    "\n",
    "# num_points = len(df.partitions[:num_partitions])\n",
    "t00 = time.time()\n",
    "for zip_gdf in [zips_1, zips_10, zips_100]:#, zips_1000, zips_10000, zips_all]:\n",
    "    num_polygons.append(len(zip_gdf))\n",
    "    t0 = time.time()\n",
    "    # convert zip_codes to geohashes\n",
    "    geohashes = list(zip_gdf.geometry.apply(polygon_to_geohashes, \n",
    "                                       precision=geohash_precision,\n",
    "                                       inner=False)\\\n",
    "                                .agg(lambda x: reduce(set.union, x)))\n",
    "    \n",
    "    # get points which match geohashes\n",
    "    dfs = []\n",
    "    for geohash in geohashes:\n",
    "        dfs.append(df.loc[geohash])\n",
    "    geohash_pts = dd.concat(dfs, axis=0)#.compute()\n",
    "    \n",
    "    # do point in polygon for exact_match\n",
    "    rdf = geohash_pts.map_partitions(spatial_join, zip_codes_gdf=zip_gdf).compute()    \n",
    "    time_sec.append(time.time() - t0)\n",
    "    \n",
    "    num_result_points.append(len(rdf))\n",
    "    print(f'num_polygons[-1]: {num_polygons[-1]}, time_sec[-1]: {time_sec[-1]:.0f} s')\n",
    "\n",
    "results_df = pd.DataFrame({'num_polygons': num_polygons,\n",
    "                           'num_points': num_points,\n",
    "                           'num_result_points': num_result_points,\n",
    "                           'sort_time_sec': 0,\n",
    "                           'time_min': np.asarray(time_sec)/60,\n",
    "                           'total_points': total_points})\n",
    "results_df['projected_total_time_hr'] = results_df.time_min*total_points/num_points/60                           \n",
    "results_df.to_csv(f'{datetime.now()}_unsorted_results_df.csv')\n",
    "results_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
