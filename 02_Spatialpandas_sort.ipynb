{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02 Spatialpandas spatial sort of point data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spatialpandas import GeoSeries, GeoDataFrame\n",
    "from spatialpandas.io import to_parquet, read_parquet\n",
    "import spatialpandas\n",
    "import dask.dataframe as dd\n",
    "from distributed import LocalCluster, Client\n",
    "import datashader as ds\n",
    "import holoviews as hv\n",
    "from datetime import datetime\n",
    "hv.extension('bokeh')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up the cluster for running Dask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster = LocalCluster(#silence_logs=logging.ERROR,\n",
    "                       dashboard_address=':3737',\n",
    "                       n_workers=4,\n",
    "                       threads_per_worker=2,\n",
    "                       memory_limit='3 GB')\n",
    "\n",
    "client = Client(cluster)\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE: Process 1, 10, 100 zip codes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spatially sort the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set path to contiguous us point dataset\n",
    "cont_us_path = \"/home/kcpevey/scipy/git/data/contiguous_us.parquet\"\n",
    "# set output path for the spatially sorted data\n",
    "savepath = '/work/kcpevey/scipy/us_cont_spatiallysorted.parquet'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_start = datetime.now()\n",
    "print(f'Started: {time_start}')\n",
    "    \n",
    "print('Read parquet')\n",
    "# ddf11 = dd.read_parquet(cont_us_path_part, engine='pyarrow', gather_statistics=False)\n",
    "ddf = dd.read_parquet(cont_us_path)\n",
    "\n",
    "# Create a spatialpandas PointArray from longitude and latitude\n",
    "print('Create a spatialpandas PointArray from longitude and latitude')\n",
    "df = ddf.map_partitions(\n",
    "    lambda df: GeoDataFrame(dict(\n",
    "        geometry=PointArray(df[['longitude', 'latitude']]),\n",
    "        **{col: df[col] for col in df.columns}\n",
    "    ))\n",
    ")\n",
    "\n",
    "# Create spatially partitioned parquet file\n",
    "print('Create spatially partitioned parquet file')\n",
    "ddf_packed = df.pack_partitions(npartitions=df.npartitions, shuffle='disk')\n",
    "ddf_packed.to_parquet(savepath)\n",
    "\n",
    "time_end = datetime.now()\n",
    "total_time = time_end - time_start\n",
    "print(f'Total processing time: {total_time}')\n",
    "print('Complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save some timing information for reporting\n",
    "with open(f'spatial_sort_time-{datetime.now()}.csv', 'w') as f:\n",
    "    f.write(f'time_min,npartitions\\n{dt/60},{df.npartitions}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following is straight out of the spatialpandas overview example https://github.com/holoviz/spatialpandas/blob/master/examples/Overview.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View the partitions before and after sorting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "def plot_partitions(ddf):\n",
    "    # Get divisions array\n",
    "    divs = np.array(ddf.divisions)[:-1]\n",
    "    \n",
    "    # Add categorical \"partition\" column\n",
    "    ddf2 = ddf.map_partitions(\n",
    "        lambda df: df.assign(\n",
    "            partition=pd.Categorical(np.searchsorted(divs, df.index, side=\"right\"))\n",
    "        )\n",
    "    ).compute()\n",
    "    \n",
    "    # Create Datashader image, coloring countries by partition\n",
    "    cvs = ds.Canvas(plot_width=650, plot_height=400)\n",
    "    agg = cvs.points(ddf2, geometry='geometry', agg=ds.count_cat('partition'))\n",
    "    return ds.transfer_functions.shade(agg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_partitions(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_partitions(ddf_packed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Open the data to ensure that it loads properly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spatialpandas.io.read_parquet_dask(savepath)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shut down the cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster.scale(0)\n",
    "client.close()\n",
    "cluster.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
