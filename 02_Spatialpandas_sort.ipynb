{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spatialpandas import GeoSeries, GeoDataFrame\n",
    "from spatialpandas.io import to_parquet, read_parquet\n",
    "from shapely.geometry import Polygon, Point\n",
    "import spatialpandas\n",
    "import dask.dataframe as dd\n",
    "import geopandas as gpd\n",
    "import time\n",
    "import dask.dataframe as dd\n",
    "from distributed import LocalCluster, Client\n",
    "from spatialpandas.geometry import PointArray\n",
    "import datashader as ds\n",
    "import holoviews as hv\n",
    "from datetime import datetime\n",
    "hv.extension('bokeh')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up the cluster for running Dask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster = LocalCluster(#silence_logs=logging.ERROR,\n",
    "                       dashboard_address=':3737',\n",
    "                       n_workers=4,\n",
    "                       threads_per_worker=2,\n",
    "                       memory_limit='3 GB')\n",
    "\n",
    "\n",
    "client = Client(cluster)\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE: Process 1, 10, 100 zip codes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following is adapted from https://github.com/Quansight/datum/blob/270266b83eb7696fdac9b8b670137ff500b15b92/datum/api/preprocess.py#L77"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# contiguous us full dataset\n",
    "cont_us_path = \"/home/kcpevey/scipy/git/data/contiguous_us.parquet\"\n",
    "\n",
    "# contiguous us single partition (for testing)\n",
    "cont_us_path_part = \"/home/kcpevey/scipy/git/data/contiguous_us.parquet/part.100.parquet\"\n",
    "\n",
    "\n",
    "tempdir_format = \"/work/kcpevey/scipy/tempdir/{partition:04d}.parquet\"\n",
    "output_path = \"/work/kcpevey/scipy/spatial_sorted\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_start = datetime.now()\n",
    "print(f'Started: {time_start}')\n",
    "    \n",
    "print('Read parquet')\n",
    "ddf11 = dd.read_parquet(cont_us_path_part, engine='pyarrow', gather_statistics=False)\n",
    "\n",
    "# Create a spatialpandas PointArray from longitude and latitude\n",
    "print('Create a spatialpandas PointArray from longitude and latitude')\n",
    "ddf3 = ddf11.map_partitions(\n",
    "    lambda df: GeoDataFrame(dict(\n",
    "        position=PointArray(df[['longitude', 'latitude']]),\n",
    "        **{col: df[col] for col in df.columns}\n",
    "    ))\n",
    ")\n",
    "\n",
    "ddf3 = ddf3.repartition(npartitions=5) # I was playing with this to see if it \n",
    "# would distribute the single partition to a more acceptable size. It doesn't \n",
    "# seem to help\n",
    "\n",
    "# Create spatially partitioned parquet file from ddf3\n",
    "print('Create spatially partitioned parquet file from ddf3')\n",
    "packed_path = f'{output_path}.parquet'\n",
    "npartitions = 4\n",
    "ddf_packed = ddf3.pack_partitions_to_parquet(\n",
    "    packed_path,\n",
    "    npartitions=npartitions,\n",
    "    tempdir_format=tempdir_format,\n",
    ")\n",
    "\n",
    "time_end = datetime.now()\n",
    "total_time = time_end - time_start\n",
    "print(f'Total processing time: {total_time}')\n",
    "print('Complete')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following is straight out of the spatialpandas overview example https://github.com/holoviz/spatialpandas/blob/master/examples/Overview.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View the partitions before and after sorting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "def plot_partitions(ddf):\n",
    "    # Get divisions array\n",
    "    divs = np.array(ddf.divisions)[:-1]\n",
    "    \n",
    "    # Add categorical \"partition\" column\n",
    "    ddf2 = ddf.map_partitions(\n",
    "        lambda df: df.assign(\n",
    "            partition=pd.Categorical(np.searchsorted(divs, df.index, side=\"right\"))\n",
    "        )\n",
    "    ).compute()\n",
    "    \n",
    "    # Create Datashader image, coloring countries by partition\n",
    "    cvs = ds.Canvas(plot_width=650, plot_height=400)\n",
    "    agg = cvs.points(ddf2, geometry='geometry', agg=ds.count_cat('partition'))\n",
    "    return ds.transfer_functions.shade(agg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_partitions(ddf3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_partitions(ddf_packed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster.scale(0)\n",
    "client.close()\n",
    "cluster.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
