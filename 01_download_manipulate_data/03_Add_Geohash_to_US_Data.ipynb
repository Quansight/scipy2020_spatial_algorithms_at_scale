{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import time\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "import geohash\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import dask\n",
    "import dask.dataframe as dd\n",
    "from distributed import LocalCluster, Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare a local dask cluster\n",
    "cluster = LocalCluster(dashboard_address=':8790',\n",
    "                       n_workers=4,\n",
    "                       threads_per_worker=2,\n",
    "                       memory_limit='3 GB')\n",
    "client = Client(cluster)\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up data paths\n",
    "base_path = Path().cwd().parent\n",
    "data_dir = base_path.joinpath('data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in the point data\n",
    "df = dd.read_parquet(data_dir.joinpath('contiguous_us.parquet'), engine='pyarrow')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_geohash(df):\n",
    "    \"\"\"Calculate the geohashes\n",
    "    map function for distributed processing\"\"\"\n",
    "    # add a dummy column if necessary \n",
    "    if 'geohash' not in df.columns:\n",
    "        df['geohash'] = ''\n",
    "    lat90indices = (df.latitude == 90) \n",
    "    df.loc[lat90indices, 'geohash'] = 'zzzzzzzzzzzz' # geohash.encode fails if lat==90\n",
    "    valid_indices = (df.longitude.between(-180, 180)) & (df.latitude >= -90) & (df.latitude < 90)\n",
    "    df.loc[valid_indices, 'geohash'] = df.loc[valid_indices, :].apply(lambda row: geohash.encode(row.latitude, row.longitude), axis=1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = time.time()\n",
    "# apply the geohashing function to each partition of data\n",
    "df = df.map_partitions(calculate_geohash, meta={'latitude': float, 'longitude': float, 'geohash': 'object'})\n",
    "# repartition the data\n",
    "df_repartition = df.repartition(npartitions=200)\n",
    "# save to parquet file\n",
    "df_repartition.to_parquet(data_dir.joinpath('contiguous_us_w_geohash.parquet'), engine='pyarrow', compression=None)\n",
    "dt_hr = (time.time() - t0)/60/60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save out timing info\n",
    "with open(f'us_geohash_time_{datetime.now()}.csv', 'w') as f:\n",
    "    f.write(f'dt_hr,{dt_hr}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# release the dask workers\n",
    "cluster.scale(0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
