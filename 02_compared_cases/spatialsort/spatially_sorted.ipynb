{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dask Geohash Sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import time\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from shapely.geometry import Polygon, box\n",
    "from polygon_geohasher.polygon_geohasher import polygon_to_geohashes, geohashes_to_polygon\n",
    "import geohash\n",
    "from functools import reduce\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import dask.dataframe as dd\n",
    "from distributed import LocalCluster, Client\n",
    "\n",
    "import spatialpandas as spd\n",
    "from spatialpandas.io import read_parquet, read_parquet_dask\n",
    "import geopandas as gpd\n",
    "from pathlib import Path\n",
    "from distributed import LocalCluster, Client\n",
    "import numpy as np\n",
    "import dask.dataframe as dd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up data paths\n",
    "base_path = Path().cwd().parent.parent\n",
    "data_dir = base_path.joinpath('data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create local dask cluster\n",
    "cluster = LocalCluster(#silence_logs=logging.ERROR,\n",
    "                       dashboard_address=':8790',\n",
    "                       n_workers=4,\n",
    "                       threads_per_worker=2,\n",
    "                       memory_limit='3 GB')\n",
    "client = Client(cluster)\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load spatially sorted us data\n",
    "spatial_sort_path = data_dir.joinpath('us_cont_spatiallysorted.parquet')\n",
    "df = read_parquet_dask(spatial_sort_path)\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len_df = len(df)\n",
    "len_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load various size subsets of the zip code data as spatialpandas.geodataframes\n",
    "zips_1 = gpd.read_file(data_dir.joinpath('zip_codes', 'zips_1.geojson')).loc[:, ['geometry']]\n",
    "zips_1 = spd.geodataframe.GeoDataFrame(zips_1, geometry='geometry')\n",
    "zips_10 = gpd.read_file(data_dir.joinpath('zip_codes', 'zips_10.geojson')).loc[:, ['geometry']]\n",
    "zips_10 = spd.geodataframe.GeoDataFrame(zips_10, geometry='geometry')\n",
    "zips_100 = gpd.read_file(data_dir.joinpath('zip_codes', 'zips_100.geojson').loc[:, ['geometry']]\n",
    "zips_100 = spd.geodataframe.GeoDataFrame(zips_100, geometry='geometry')\n",
    "zips_1000 = gpd.read_file(data_dir.joinpath('zip_codes', 'zips_1000.geojson')).loc[:, ['geometry']]\n",
    "zips_1000 = spd.geodataframe.GeoDataFrame(zips_1000, geometry='geometry')\n",
    "zips_10000 = gpd.read_file(data_dir.joinpath('zip_codes', 'zips_10000.geojson')).loc[:, ['geometry']]\n",
    "zips_10000 = spd.geodataframe.GeoDataFrame(zips_10000, geometry='geometry')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "total_points = len_df\n",
    "num_partitions = df.npartitions\n",
    "num_polygons = []\n",
    "time_sec = []\n",
    "num_result_points = []\n",
    "num_points = len_df\n",
    "\n",
    "t00 = time.time()\n",
    "for zip_gdf in [zips_1, zips_10, zips_100, zips_1000, zips_10000]:\n",
    "    num_polygons.append(len(zip_gdf))\n",
    "    t0 = time.time()\n",
    "    \n",
    "    rdf = spd.sjoin(df, zip_gdf, how='inner').compute()\n",
    "\n",
    "    time_sec.append(time.time() - t0)\n",
    "    num_result_points.append(len(rdf))\n",
    "    print(f'num_polygons[-1]: {num_polygons[-1]}, time_sec[-1]: {time_sec[-1]:.0f} s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save summary info to file\n",
    "results_df = pd.DataFrame({'num_polygons': num_polygons,\n",
    "                           'num_points': num_points,\n",
    "                           'num_result_points': num_result_points,\n",
    "                           'time_min': np.asarray(time_sec)/60})                      \n",
    "results_df.to_csv(f'spatially_sorted_results_{datetime.now()}.csv')\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# release the dask workers\n",
    "cluster.scale(0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
