{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dask Geohash Sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import time\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from shapely.geometry import Polygon, box\n",
    "from polygon_geohasher.polygon_geohasher import polygon_to_geohashes, geohashes_to_polygon\n",
    "import geohash\n",
    "from functools import reduce\n",
    "from math import ceil\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import dask.dataframe as dd\n",
    "from distributed import LocalCluster, Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create local dask cluster\n",
    "cluster = LocalCluster(#silence_logs=logging.ERROR,\n",
    "                       dashboard_address=':8790',\n",
    "                       n_workers=4,\n",
    "                       threads_per_worker=2,\n",
    "                       memory_limit='3 GB')\n",
    "client = Client(cluster)\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up data paths\n",
    "base_path = Path().cwd().parent.parent\n",
    "data_dir = base_path.joinpath('data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load contiguous us data\n",
    "df = dd.read_parquet(data_dir.joinpath('contiguous_us_geohash4_sorted.parquet'))\n",
    "display(df.head(2))\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load various size subsets of the zip code data as geodataframes\n",
    "zips_1 = gpd.read_file(data_dir.joinpath('zip_codes', 'zips_1.geojson')).loc[:, ['geometry']]\n",
    "zips_10 = gpd.read_file(data_dir.joinpath('zip_codes', 'zips_10.geojson')).loc[:, ['geometry']]\n",
    "zips_100 = gpd.read_file(data_dir.joinpath('zip_codes', 'zips_100.geojson')).loc[:, ['geometry']]\n",
    "zips_1000 = gpd.read_file(data_dir.joinpath('zip_codes', 'zips_1000.geojson')).loc[:, ['geometry']]\n",
    "zips_10000 = gpd.read_file(data_dir.joinpath('zip_codes', 'zips_10000.geojson')).loc[:, ['geometry']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter function\n",
    "empty_df = pd.DataFrame([], columns=['latitude', 'longitude'])\n",
    "def spatial_join(large_data_df, zip_codes_gdf):\n",
    "    \"\"\"map function for distributed processing\"\"\"\n",
    "    # catch for empty subset dataframes\n",
    "    if large_data_df.empty:\n",
    "        return empty_df\n",
    "    # create geodataframe\n",
    "    crs = \"epsg:4326\"\n",
    "    large_data_gdf = gpd.GeoDataFrame(large_data_df,\n",
    "                                      geometry=gpd.points_from_xy(large_data_df.longitude,\n",
    "                                                                  large_data_df.latitude),\n",
    "                                      crs=crs)\n",
    "    # perform the spatial join\n",
    "    rdf = gpd.sjoin(large_data_gdf,\n",
    "                    zip_codes_gdf,\n",
    "                    how='inner',\n",
    "                    op='within').drop(['index_right', 'geometry'], axis=1)\n",
    "    if rdf.empty:\n",
    "        return empty_df\n",
    "    return rdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "total_points = len(df)\n",
    "num_partitions = df.npartitions\n",
    "geohash_precision = 4\n",
    "num_polygons = []\n",
    "time_sec = []\n",
    "num_result_points = []\n",
    "num_points = len(df.partitions[:num_partitions])\n",
    "\n",
    "t00 = time.time()\n",
    "# loop over the subsets of zip codes\n",
    "for zip_gdf in [zips_1, zips_10, zips_100, zips_1000]:#, zips_10000]:\n",
    "    num_polygons.append(len(zip_gdf))\n",
    "    t0 = time.time()\n",
    "    \n",
    "    # get unique geohashes from data (could be saved)\n",
    "    unique_geohashes = df.index.unique().compute()\n",
    "    \n",
    "    # convert zip_codes to geohashes\n",
    "    geohash_df = zip_gdf.geometry.apply(polygon_to_geohashes, \n",
    "                                                   precision=geohash_precision,\n",
    "                                                   inner=False)\n",
    "    rdfs = []\n",
    "    for polygon_index, geohash_set in geohash_df.iteritems():\n",
    "        zip_geohashes = list(geohash_set.intersection(unique_geohashes.values))  # filter out geohashes not in data \n",
    "        possible_interior_pts = df.loc[zip_geohashes]\n",
    "        rdfs.append(possible_interior_pts.map_partitions(spatial_join, zip_codes_gdf=zip_gdf.loc[polygon_index:polygon_index]))\n",
    "    rdf = dd.concat(rdfs).compute()\n",
    "\n",
    "    time_sec.append(time.time() - t0)\n",
    "    num_result_points.append(len(rdf))\n",
    "    print(f'num_polygons[-1]: {num_polygons[-1]}, time_sec[-1]: {time_sec[-1]:.0f} s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save summary info to file\n",
    "results_df = pd.DataFrame({'num_polygons': num_polygons,\n",
    "                           'num_points': num_points,\n",
    "                           'num_result_points': num_result_points,\n",
    "                           'time_min': np.asarray(time_sec)/60})                       \n",
    "results_df.to_csv(f'{datetime.now()}_geohash_sorted_results.csv')\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# release the dask workers\n",
    "cluster.scale(0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
