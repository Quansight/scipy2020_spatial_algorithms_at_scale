{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dask Geohash Sorted No sjoin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import time\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from shapely.geometry import Polygon, box\n",
    "from polygon_geohasher.polygon_geohasher import polygon_to_geohashes, geohashes_to_polygon\n",
    "import geohash\n",
    "from functools import reduce\n",
    "from math import ceil\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import dask.dataframe as dd\n",
    "from distributed import LocalCluster, Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create local dask cluster\n",
    "cluster = LocalCluster(#silence_logs=logging.ERROR,\n",
    "                       dashboard_address=':8790',\n",
    "                       n_workers=4,\n",
    "                       threads_per_worker=2,\n",
    "                       memory_limit='3 GB')\n",
    "client = Client(cluster)\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up data paths\n",
    "base_path = Path().cwd().parent.parent\n",
    "data_dir = base_path.joinpath('data')\n",
    "contiguous_us_bounding_box = box(-124.848974, 24.396308, -66.885444, 49.384358)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load contiguous us data\n",
    "df = dd.read_parquet(data_dir.joinpath('contiguous_us_geohash4_sorted.parquet'))\n",
    "display(df.head(2))\n",
    "len_df = len(df)\n",
    "len_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load various size subsets of the zip code data as geodataframes\n",
    "zips_1 = gpd.read_file(data_dir.joinpath('zip_codes', 'zips_1.geojson')).loc[:, ['geometry']]\n",
    "zips_10 = gpd.read_file(data_dir.joinpath('zip_codes', 'zips_10.geojson')).loc[:, ['geometry']]\n",
    "zips_100 = gpd.read_file(data_dir.joinpath('zip_codes', 'zips_100.geojson')).loc[:, ['geometry']]\n",
    "zips_1000 = gpd.read_file(data_dir.joinpath('zip_codes', 'zips_1000.geojson')).loc[:, ['geometry']]  \n",
    "zips_10000 = gpd.read_file(data_dir.joinpath('zip_codes', 'zips_10000.geojson')).loc[:, ['geometry']]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "total_points = len_df\n",
    "num_partitions = df.npartitions\n",
    "geohash_precision = 4\n",
    "num_polygons = []\n",
    "time_sec = []\n",
    "num_result_points = []\n",
    "num_points = len(df.partitions[:num_partitions])\n",
    "\n",
    "t00 = time.time()\n",
    "for zip_gdf in [zips_1, zips_10, zips_100, zips_1000, zips_10000]:\n",
    "    num_polygons.append(len(zip_gdf))\n",
    "    t0 = time.time()\n",
    "    \n",
    "    # get unique geohashes from data (could be precomputed and saved)\n",
    "    unique_geohashes = df.index.unique().compute()\n",
    "    \n",
    "    # convert zip_codes to geohashes\n",
    "    geohash_df = zip_gdf.geometry.apply(polygon_to_geohashes, \n",
    "                                                   precision=geohash_precision,\n",
    "                                                   inner=False)#.apply(list)#.explode().to_frame()\n",
    "    \n",
    "    geohash_set = zip_geohashes = geohash_df.agg(lambda x: reduce(set.union, x))\n",
    "    rdfs = []\n",
    "    zip_geohashes = list(geohash_set.intersection(unique_geohashes.values))  # filter out geohashes not in data \n",
    "    possible_interior_pts = df.loc[zip_geohashes]\n",
    "    rdfs.append(possible_interior_pts)\n",
    "    rdf = dd.concat(rdfs).compute()\n",
    "\n",
    "    time_sec.append(time.time() - t0)\n",
    "    num_result_points.append(len(rdf))\n",
    "    print(f'num_polygons[-1]: {num_polygons[-1]}, time_sec[-1]: {time_sec[-1]:.0f} s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save summary info to file\n",
    "results_df = pd.DataFrame({'num_polygons': num_polygons,\n",
    "                           'num_points': num_points,\n",
    "                           'num_result_points': num_result_points,\n",
    "                           'time_min': np.asarray(time_sec)/60})                      \n",
    "results_df.to_csv(f'geohash_sorted_no_sjoin_results_{datetime.now()}.csv')\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# release the dask workers\n",
    "cluster.scale(0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
