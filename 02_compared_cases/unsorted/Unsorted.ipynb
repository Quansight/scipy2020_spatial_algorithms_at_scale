{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unsorted Point in Polygon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import time\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import dask.dataframe as dd\n",
    "from distributed import LocalCluster, Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up data paths\n",
    "base_path = Path().cwd().parent.parent\n",
    "data_dir = base_path.joinpath('data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create local dask cluster\n",
    "cluster = LocalCluster(#silence_logs=logging.ERROR,\n",
    "                       dashboard_address=':8790',\n",
    "                       n_workers=4,\n",
    "                       threads_per_worker=2,\n",
    "                       memory_limit='5 GB')\n",
    "client = Client(cluster)\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load contiguous us data\n",
    "df = dd.read_parquet(data_dir.joinpath('contiguous_us_w_geohash.parquet'), columns=['latitude', 'longitude'], engine='pyarrow')\n",
    "display(df.head(2))\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load various size subsets of the zip code data as geodataframes\n",
    "zips_1 = gpd.read_file(data_dir.joinpath('zip_codes', 'zips_1.geojson')).loc[:, ['geometry']]\n",
    "zips_10 = gpd.read_file(data_dir.joinpath('zip_codes', 'zips_10.geojson')).loc[:, ['geometry']]\n",
    "zips_100 = gpd.read_file(data_dir.joinpath('zip_codes', 'zips_100.geojson')).loc[:, ['geometry']]\n",
    "zips_1000 = gpd.read_file(data_dir.joinpath('zip_codes', 'zips_1000.geojson')).loc[:, ['geometry']]\n",
    "# zips_10000 = gpd.read_file(data_dir.joinpath('zip_codes', 'zips_10000.geojson')).loc[:, ['geometry']]  # fails for the unsorted case"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Point in Polygon Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter function\n",
    "def spatial_join(large_data_df, zip_codes_gdf):\n",
    "    crs = \"epsg:4326\"\n",
    "    large_data_gdf = gpd.GeoDataFrame(large_data_df,\n",
    "                                      geometry=gpd.points_from_xy(large_data_df.longitude,\n",
    "                                                                  large_data_df.latitude),\n",
    "                                      crs=crs)\n",
    "    return gpd.sjoin(large_data_gdf, zip_codes_gdf, how='inner', op='within')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_points = len(df)\n",
    "num_partitions = df.npartitions\n",
    "num_polygons = []\n",
    "time_sec = []\n",
    "num_result_points = []\n",
    "num_points = None\n",
    "\n",
    "num_points = len(df.partitions[:num_partitions])\n",
    "t00 = time.time()\n",
    "for zip_gdf in [zips_1, zips_10, zips_100]:#, zips_1000, zips_10000, zips_all]:\n",
    "    num_polygons.append(len(zip_gdf))\n",
    "    t0 = time.time()\n",
    "    rdf = df.partitions[:num_partitions].map_partitions(spatial_join, zip_codes_gdf=zip_gdf).compute()\n",
    "    time_sec.append(time.time() - t0)\n",
    "    num_result_points.append(len(rdf))\n",
    "    print(f'num_polygons[-1]: {num_polygons[-1]}, time_sec[-1]: {time_sec[-1]:.0f} s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save summary info to file\n",
    "results_df = pd.DataFrame({'num_polygons': num_polygons,\n",
    "                           'num_points': num_points,\n",
    "                           'num_result_points': num_result_points,\n",
    "                           'sort_time_sec': 0,\n",
    "                           'time_min': np.asarray(time_sec)/60})                          \n",
    "results_df.to_csv(f'unsorted_results_df_{datetime.now()}.csv')\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# release the dask workers\n",
    "cluster.scale(0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
